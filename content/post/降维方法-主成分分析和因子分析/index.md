+++
date = '2024-11-27T12:09:07+08:00'
draft = true
title = '降维方法 主成分分析和因子分析'

+++

初次发布于[我的个人文档](https://colablack.github.io)。（每次都是个人文档优先发布哦）

本文简要介绍一下主成分分析和因子分析的原理，但是不涉及具体代码实现。这是因为现在已经有很多现成的软件或库实现了这两个算法，读者只需要一两句简单的命令就可以使用了，所以没有必要在这里讲解。而且你可能会在Python R MATLAB SPSS等多种不同的软件中使用，无论选哪个软件的代码实现都没有特别强的代表性。

### 主成分分析

如果你手上有一组数据，例如是大家的语文数学英语成绩。但是现在有一个问题，咱们的试卷出得有那么一点点不好，大家的成绩都集中在一起了，也就是试卷的区分度不大。现在，我们有没有办法补救呢？

注意：这只是一个例子而已，自然是不考虑我们进行各种变换之后的现实问题，例如这样搞成绩会不公平啊什么的。

总之，我们的核心问题是，有没有办法对现有数据进行变换，使得数据的每一个个体尽可能被分开。

这就是主成分分析的一个可以选择的切入点。

那我们要选择什么样的变换呢？以及有没有办法将**一个群体之间的不同个体距离**拉远。

##### 以p维正态分布为例进行可行性探索

嗯，对我们先拿p维正态分布探索一下我们想法的可行性。

我们假设p维随机向量X服从协方差阵为$\Sigma$，均值向量为$\mu$的p维正态分布。

那么X的概率密度函数就是$P(x)=\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$，我们来观察其概率密度等高线，显然，这里只有e的指数是变量，所以概率密度等高线满足：

$$-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)=C_1$$

也就是$(x-\mu)^T\Sigma^{-1}(x-\mu)=C_2$

我们可以对$\Sigma $进行谱分解。

> 谱分解说明：
>
> 根据线性代数的知识我们可以知道，任意实对称阵A可以正交相似对角化，即
>
> $\forall 实对称矩阵A,\exist正交阵Q和对角阵\Lambda，使得A=Q\Lambda Q^T$
>
> 如果我们已知A的特征值$\lambda_1,\lambda_2,...,\lambda_n$和对应的特征向量$e_1,e_2,...,e_n$，则
>
> Q=($e_1,e_2,...,e_n$)，$\Lambda =diag(\lambda_1,\lambda_2,...,\lambda_n)$
>
> 这意味着:
>
> $$A=Q\Lambda Q^T$$
>
> $$=(e_1,e_2,...,e_n)diag(\lambda_1,\lambda_2,...,\lambda_n)(e_1,e_2,...,e_n)^T$$
>
> $$=(\lambda_1 e_1,\lambda_2 e_2,...,\lambda_n e_n)(e_1,e_2,...,e_n)^T$$
>
> $$=\lambda_1 e_1 e_1^T + \lambda_2 e_2 e_2^T +...+ \lambda_n e_n e_n^T$$
>
> $$=\sum_{i=1}^n\lambda_i e_i e_i^T$$
>
> 这就是谱分解了。

我们假设$\Sigma$的特征值为$\lambda_1,\lambda_2,...,\lambda_p$，对应的特征向量为$e_1,e_2,...,e_p$，那么$\Sigma$就可以谱分解为

$\Sigma=\sum_{i=1}^p\lambda_i e_i e_i^T$，将他代入概率密度等高线方程就有，

$(x-\mu)^T{(\sum_{i=1}^p\lambda_i e_i e_i^T)}^{-1}(x-\mu)=C_2$

也就是，

$$\sum_{i=1}^p\frac{[e_i(x-\mu)]^T[e_i(x-\mu)]}{C_2}=1$$

这是p维的类似椭圆的方程，当p=2时这就是椭圆方程。

这意味着，概率密度等高线是同样有着长轴和短轴。

因而，这意味着如果我们将原始变量X进行正交变换将坐标轴旋转到长轴上就可以达成我们的目标将**一个群体之间的不同个体距离拉远**。

##### 演算

接下来，我们有了方向就可以进行推演了。

我们要将原始变量X进行正交变换得到新的一组变量，这从几何看就是进行坐标轴旋转。

总之，从代数角度看就是，设p维随机向量X=$(x_1,x_2,...,x_p)$的协方差阵为$\Sigma$。

那么我们就是要找一组新的变量Z=$(z_1,z_2,...,z_p)$使得（新变量被称为主成分）

$$z_1=a_{11}x_1+a_{12}x_2+...+a_{1p}x_p=a_1^TX$$

$$z_2=a_{21}x_1+a_{22}x_2+...+a_{2p}x_p=a_2^TX$$

...

$$z_p=a_{p1}x_1+a_{p2}x_2+...+a_{pp}x_p=a_p^TX$$

而此时，$var(z_j)=a_j^T\Sigma a_j,cov(z_j,z_k)=a_j^T\Sigma a_k$

我们前面说了，我们希望将**一个群体之间的不同个体距离拉远**，也就是要最大化新变量Z的方差，与此同时我们自然希望各个新变量之间无关也就是：

最大化$var(z_j)$，希望$cov(z_j,z_k)=0$

对于$z_1$来说就是希望最大化$a_1^T\Sigma a_1$，但是显然我们可以通过无限扩大$a_1$的长度来实现最大化$z_1$的方差，这是我们不期望看到的。

所以我们再额外要求$z_1$的长度是1即$a_1^Ta_1=1$。

这样的话其实我们就是在最大化$a_1^T\Sigma a_1=\frac{a_1^T\Sigma a_1}{a_1^Ta_1}$（注意哦，现在分母为1所以除了等于没除）。

类似地，我们对$z_2$会要求$a_2^Ta_2=1$，并且$cov(z_2,z_1)=a_2^T\Sigma a_1=0$

最大化$a_2^T\Sigma a_2$。

以此类推，但是到最后一个变量$z_p$我们只能要求最小化$a_p^T\Sigma a_p$了，因为这个对应的是前面说的高维椭圆的短轴，是最小的。

那么，怎么进行最小化呢？

一般的教材这里就是上拉格朗日乘数法了，计算比较复杂我就不说了。给个结论吧。

> $\forall a \in R^p,\Sigma \in M_p且\Sigma为对称矩阵。$
>
> 设$(\lambda_j,e_j)$为$\Sigma$的特征值、单位特征向量。
>
> 那么$$a\neq 0, a⊥e_1,e_2,...,e_{j-1},$$
>
> 则$$max \frac{a^T\Sigma a}{a^T a}=\lambda_j在a=e_j时取到最大。$$

利用上述结论就可以知道，X的第j个主成分$z_j=e_j^TX$

且$var(z_j)=e_j^T\Sigma e_j$

注意$(\lambda_j,e_j)$为$\Sigma$的特征值、单位特征向量。

所以$\Sigma e_j =\lambda_j e_j,e_j^Te_j=1$

因而$var(z_j)=e_j^T\Sigma e_j=e_j^T \lambda_j e_j=\lambda_j e^T_je_j=\lambda_j$。

所以，X的第j个主成分$z_j$是其协方差矩阵$\Sigma$的第j个单位特征向量乘以原始变量X，并且第j个主成分的方差就是$\Sigma$第j个特征值$\lambda_j$。

这就是主成分分析的结论。

更进一步的，如果我们对原始变量进行标准化然后再进行主成分分析，可以证明这相当于对原始变量的相关系数矩阵R进行对应的主成分分析。

##### 降维

从上面的推导我们可以看到对p维向量进行主成分分析只能得到p个主成分，似乎不能降维啊。那么我们一般说的降维是怎么回事？

前面我们知道，第j个主成分的方差就是$\Sigma$第j个特征值$\lambda_j$。

如果我们将全部的主成分的方差求和，那就是对$\Sigma$全部的特征值的求和，也就是$\Sigma$的迹，也就是X各个分量的方差的和。

所以到现在为止我们还没有损失任何一点点方差。

如果降维的话就会损失方差了，这是因为所谓的降维就是将各个特征值从大到小排列，然后去掉比较小的特征值和对应的主成分。

这样的话就会损失方差了，也就损失了部分信息。这就是所谓的利用主成分分析进行降维。

### 因子分析

那么因子分析是什么？

还是看学生成绩数据吧，从学生的成绩上我们可以看到，优秀的学生似乎各科成绩都很好。也许你还会发现，各科成绩高度相关，这意味着他们可能由某一个潜在变量决定（智商）。

因子分析就是由原始数据寻找这样的潜变量。

由于潜变量的数量往往少于原始变量的数量，所以因子分析也是一种降维方法。

因子分析建立了一个**因子模型**，它认为原始变量Y是各个潜变量的线性组合，即

$$Y_i=l_{i1}F_1+l_{i2}F_2+...+l_{im}F_m+\epsilon_i$$

其中，$F_j$是潜变量也叫**公共因子**，我们假设有m个，当然一般要求m不大于原始变量的个数p。

系数$l_ij$被称为**因子载荷**，$Y_i$则是原始变量而$\epsilon_i$是类似误差的**特殊因子**。

我们还对公共因子提了一些基础的要求，首先$F_i,F_j$不相关（正交），$F_i,\epsilon_j、\epsilon_i,\epsilon_j$不相关，来保持各个变量之间的独立性。

当然，因子模型用矩阵表示更简洁，就是

$Y=AF+\epsilon$

那前面说的那些要求用矩阵表示就是：

- $m \le p$
- cov(F,$\epsilon$)=0
- $D_F=var(F)=单位阵I_m$
- $D_\epsilon=var(\epsilon)=diag(\sigma_1^2,\sigma_2^2,...,\sigma_p^2) $

而因子模型最重要的新增是协方差阵的矩阵分解：

$var(X)=\Sigma=cov(AF+\epsilon,AF+\epsilon)=Acov(F,F)A^{-1}+cov(\epsilon,\epsilon)=AA^{-1}+D_\epsilon$

##### 演算

那么如何求解因子模型中未知的A和$\epsilon$呢？

答案是利用协方差矩阵的矩阵分解：

$\Sigma=AA^{-1}+D_\epsilon$

而前面我们说过$\Sigma$可以分解为$Q\Lambda Q^T$，其中$Q=($e_1,e_2,...,e_n$)，$$\Lambda =diag(\lambda_1,\lambda_2,...,\lambda_n)$

我们再进行小小的变换，定义$\Lambda_2=(e_1\sqrt{\lambda_1},e_2\sqrt{\lambda_2},...,e_p\sqrt{\lambda_p})$，

则$\Sigma=\Lambda_2 \Lambda_2$。

当$\Sigma$的后p-m个特征值很小的时候，我们就可以忽略掉后面的项，用$\Lambda_2$的前m项估计A，从而$D_\epsilon=\Sigma-AA^{-1}$也就可以计算了。

这就是因子分析的主成分法求解。

##### 主轴因子法

除此之外因子分析还有一个常用的算法是主轴因子法，推导比较复杂我就只说思路了。

我们知道

$\Sigma=AA^{-1}+D_\epsilon$

那么如果我们先估计$D_\epsilon$在分解也可以得到A。

总思路是这样的，不过如果你去翻阅各种资料的话，可能还会遇到约相关阵的说法，

也就是先把原始变量进行标准化从而$\Sigma=原始变量的相关系数R$再定义$R^*=R-D_\epsilon=AA^{-1}$，

然后估计$R^*$再计算A和$D_\epsilon$也是可以的。

